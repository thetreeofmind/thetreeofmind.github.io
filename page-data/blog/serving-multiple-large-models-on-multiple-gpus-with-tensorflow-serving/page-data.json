{
    "componentChunkName": "component---src-templates-blog-post-js",
    "path": "/blog/serving-multiple-large-models-on-multiple-gpus-with-tensorflow-serving",
    "result": {"data":{"butterPost":{"title":"Serving multiple large ML models on multiple GPUs with Tensorflow Serving","seo_title":"Serving multiple large models on multiple GPUs with Tensorflow Serving","published":"2021-05-15T15:58:00Z","featured_image":"https://cdn.buttercms.com/0hwcq6pQfqVLVmgR1AT3","body":"<p>As machine learning problem gets more complex and training data gets more massive, the machine learning models are growing in size and variation as well. This creates bigger challenges to serve models in production. To achieve better speed, we turn our eyes to GPU. To serve bigger and more types of models, we again look for multiple GPUs. In this article, we will discuss how to serve multiple large ML models on multiple GPUs with Tensorflow Serving on a multi-GPU machine.&nbsp;</p>\n<p></p>\n<h2>What's Tensorflow Serving</h2>\n<p><a href=\"https://www.tensorflow.org/tfx/guide/serving\" rel=\"follow\">Tensorflow Serving</a> is high performance model serving framework, developed by Tensorflow, to serve machine learning models in production environment. It offers a lot of nice features including:</p>\n<ul>\n<li>Serves multiple models in a single API endpoint</li>\n<li>Supports server side request batching to boost throughput</li>\n<li>Supports model version management</li>\n</ul>\n<p>Tesorflow serving was released back in 2016 and has been around for quite a few years. New features have been continuously added along with the development of Tensorflow framework itself. With the current features it offers, you can easily use it to serve a single or multiple machine learning models on a single GPU.&nbsp;</p>\n<p>However, when it comes to use Tensorflow serving to serve multiple large ML models across multiple GPUs on a multi-GPU machine, things get a little messy and the solution is not that straightforward. Here is my experience of how to resolve it.</p>\n<p></p>\n<h2>What problem are we solving</h2>\n<p>The problem we are trying to solve is to serve multiple large machine learning models across multiple GPUs on a multi-GPU machine, with Tensorflow Serving. If your ML serving system is one of the following, then this situation probably does not apply to you:</p>\n<ol>\n<li>You only have 1 model to serve; OR</li>\n<li>You have multiple models, but total size of the models fits in a single GPU on the machine you use; OR</li>\n<li>You only use single-GPU machine</li>\n</ol>\n<p>Now, the ML serving system we are having problem with using Tensorflow Serving is:</p>\n<ol>\n<li>We have multiple models; AND</li>\n<li>Each model fits in a single GPU, but the total size of all the models do not fit in a single GPU; AND</li>\n<li>We are using a multi-GPU machine so that we can serve multiple models in multiple GPUs</li>\n</ol>\n<p>With that said, imagine that our ML system looks like the following:</p>\n<ul>\n<li>12 machine learning models, and each model is 4GB in size</li>\n<li>A multi-gpu machine (such as AWS <a href=\"https://aws.amazon.com/ec2/instance-types/g4/\" rel=\"follow\">g4dn.12xlarge EC2 machine</a>, which has 4 GPUs and each GPU is 15GB) to host the models</li>\n</ul>\n<p>Notice that in this ML system, we can't use only 1 GPU to server all 12 models, as it doesn't fit. We have to use all 4 GPUs to serve those 12 models. Let's take a look how Tensorflow Serving can be used to serve our 12 models.&nbsp;</p>\n<p></p>\n<h2>Tensorflow Serving load models on single GPU by default</h2>\n<p>If you have already been using Tensorflow Serving, then you are probably familiar with the typical ways of running Tensorflow serving server.&nbsp;</p>\n<h3>Serve via Docker</h3>\n<p>To server multiple models using Tensorflow Serving GPU docker container:</p>\n<pre class=\"language-undefined\"><code>docker run --runtime=nvidia -p 8500:8500 -p 8501:8501 \\\n--mount type=bind, source=/path/to/my_model/, target=/models/my_model \\\n--mount type=bind, source=/path/to/my/models.config, target=/models/models.config \\\n-t tensorflow/serving:latest-gpu\n--model_config_file=/models/models.config</code></pre>\n<h3>Serve via tensorflow_model_server binary:</h3>\n<p>If for some reason you can't use docker or Tensorflow Serving docker container, then you can serve multiple models using <a href=\"https://www.tensorflow.org/tfx/serving/setup\" rel=\"follow\">tensorflow_model_server binary</a> with GPU support:</p>\n<pre class=\"language-undefined\"><code>tensorflow_model_server --port=8500 --rest_api_port=8501 --model_config_file=/models/models.config</code></pre>\n<p>The problem is, Tensorflow Serving only allocate 1 GPU by default (<code>\"/GPU:0\"</code>) to load all the models, and as soon as we start our model server with all 12 models, the model server crashes. Remember that the total size of our 12 models is 48 GB, while a single GPU is only 15 GB. This will cause GPU out of memory very quickly before the model server is able to fully load all models, due to the size of all 12 models being bigger that the single GPU memory.</p>\n<p>Normally, If we are using regular Tensorflow only, then we can easily specify which GPU to load which model, and evenly split the models across GPUs, by doing the following:</p>\n<ul>\n<li><code>/GPU:0</code> will load model 0, 4, 8</li>\n<li><code>/GPU:1</code> will load model 1, 5, 9</li>\n<li><code>/GPU:2</code> will load model 2, 6, 10</li>\n<li><code>/GPU:3</code> will load model 3, 7, 11</li>\n</ul>\n<pre class=\"language-python\"><code>with tf.device('/GPU:0'):\n// load model 0, model 4, model 8\n\nwith tf.device('/GPU:1'):\n// load model 1, model 5, model 9\n\nwith tf.device('/GPU:2'):\n// load model 2, model 6, model 10\n\nwith tf.device('/GPU:3'):\n// load model 3, model 7, model 11 </code></pre>\n<p>However, with Tensorflow Serving, there is no interface or command line argument to control which GPU to load which model. At least not yet. The <span class=\"remove_line\"><span class=\"line-content\"><span class=\"s\"><code>tensorflow_model_server</code> command takes all our 12 models and by default tries to load all of them into a single GPU instead.&nbsp;</span></span></span></p>\n<p></p>\n<h2>Solution: Save our models with specific device placement when training</h2>\n<p>Although <span class=\"remove_line\"><span class=\"line-content\"><span class=\"s\"><code>tensorflow_model_server</code></span></span></span> does not offer us a way to split our models across multiple GPUs, we can, however, split the models across multiple GPUs when we save our models after training.</p>\n<p>During training process, if we don't specify a GPU device, Tensorflow will use the default GPU device (which would be <code>\"/GPU:0\"</code>) to load the graph, and when the model is saved, the device placement is not specified either. When serving the models, Tensorflow Serving, upon not seeing a specific device placement, will again choose the default GPU device (which would be <code>\"/GPU:0\"</code>) to load the models. This will not be good if the total size of our model is bigger than the single GPU memory size.</p>\n<p>By understanding that, the solution to our problem is straightforward - <strong>saving our models with specific device placement during training</strong>. A quick illustration of how that looks:</p>\n<ul>\n<li><code>/GPU:0</code> will be used to train and save model 0, 4, 8</li>\n<li><code>/GPU:1</code> will be used to train and save model 1, 5, 9</li>\n<li><code>/GPU:2</code> will be used to train and save model 2, 6, 10</li>\n<li><code>/GPU:3</code> will be used to train and save model 3, 7, 11</li>\n</ul>\n<pre class=\"language-python\"><code>with tf.device('/GPU:0'):\n// train and save model 0, model 4, model 8\n\nwith tf.device('/GPU:1'):\n// train and save model 1, model 5, model 9\n\nwith tf.device('/GPU:2'):\n// train and save model 2, model 6, model 10\n\nwith tf.device('/GPU:3'):\n// train and save model 3, model 7, model 11 </code></pre>\n<p>Now that each model graph has a specific device placement, Tensorflow Serving, when loading the models, will honor the device placement request, and only load the model on the specific GPU device. As a result of that, Tensorflow Serving will split the models across multiple GPUs. This helps us avoid Out-of-Memory problem and successfully starts the model server.</p>\n<p></p>\n<h2>Final Words</h2>\n<p>Tensorflow Serving is a great tool to serve machine learning models in production. While we hope that the tool gets improved and updated soon to natively support serving multiple large models across multiple GPUs, hopefully the solution I shared here can get you unblocked and also keep using Tensorflow Serving for your production serving system.</p>","date":"5/15/2021","meta_description":"In this article, we will discuss how to serve multiple large ML models on multiple GPUs with Tensorflow Serving on a multi-GPU machine.","tags":[{"name":"Tenforflow"},{"name":"Tensorflow Serving"},{"name":"GPU"},{"name":"Machine Learning"},{"name":"Multiple Models"},{"name":"Multiple GPUs"}]}},"pageContext":{"slug":"serving-multiple-large-models-on-multiple-gpus-with-tensorflow-serving"}},
    "staticQueryHashes": []}