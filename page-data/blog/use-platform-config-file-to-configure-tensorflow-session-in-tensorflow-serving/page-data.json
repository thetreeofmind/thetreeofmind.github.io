{
    "componentChunkName": "component---src-templates-blog-post-js",
    "path": "/blog/use-platform-config-file-to-configure-tensorflow-session-in-tensorflow-serving",
    "result": {"data":{"butterPost":{"title":"Use platform_config_file to configure Tensorflow session in Tensorflow Serving","seo_title":"Use platform_config_file to configure Tensorflow session in Tensorflow Serving","published":"2021-06-30T00:59:00Z","featured_image":"https://cdn.buttercms.com/eUkOCKorTMWH6DX083Do","body":"<p class=\"p1\">The typical way of running tensorflow serving is to use tensorflow serving docker container or tensorflow_model_server binary.<span class=\"Apple-converted-space\"> It offers a list of arguments you can pass to easily configure how you want to run the tensorflow serving server. For example, you may start your Tensorflow Serving server like this:</span></p>\n<pre class=\"language-undefined\"><code>tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n  --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME}</code></pre>\n<p class=\"p1\"><span class=\"Apple-converted-space\">To see a full list of arguments you can pass, check out the <a href=\"https://github.com/tensorflow/serving/blob/master/tensorflow_serving/model_servers/main.cc\" rel=\"follow\">source code</a>.</span></p>\n<p class=\"p1\"><span class=\"Apple-converted-space\">However, when checking out the list of arguments it provides, something is missing. The common configuration that you might typically use in Tensorflow itself to configure Tensorflow session, such <code>allow_soft_placement</code><span>&nbsp;or <code>allow_growth</code></span> are nowhere to find. This limits our ability to configure Tensorflow session according to our needs when using Tensorflow Serving. In this post, we will look at what those needs are and how to use <code>platform_config_file</code> to configure Tensorflow session to address our needs when using Tensorflow Serving.</span></p>\n<p class=\"p2\"></p>\n<h2 class=\"p2\">The missing session configurations</h2>\n<p><a href=\"https://github.com/tensorflow/serving/blob/master/tensorflow_serving/model_servers/main.cc\" rel=\"follow\">The list of arguments </a>that are currently provided in Tensorflow Serving provide a good amount of flexibility to start your TF serving server.&nbsp;However, some Tensorflow session configurations are not available, including:</p>\n<ol class=\"ol1\"></ol>\n<ul class=\"ul1\">\n<li class=\"li1\"><span class=\"s1\"></span><code>tf.config.set_soft_device_placement</code> or <code>allow_soft_placement</code> - for configuring Tensorflow session to <span>automatically choose an existing and supported device to run a Tensor operation in case the specified one doesn't exist</span></li>\n<li class=\"li1\"><span class=\"s1\"><code>tf.config.experimental.set_memory_growth</code> or </span><code>allow_growth</code> - for configuring Tensorflow session to not allocate all memory on the device but to grow as needed</li>\n<li class=\"li1\"><code><span>tf.config.experimental.set_virtual_device_configuration </span></code>or <code><span>per_process_gpu_memory_fraction</span></code> for configuring Tensorflow session to set a limit on memory usage on the device</li>\n</ul>\n<p>Why would this matter? For example, if you have a ML model that contains a bunch of tensors, some tensors may only run on a CPU device. If you want to serve your ML model with Tensorflow Serving on a GPU device, without <code>tf.config.set_soft_device_placement(True)</code>, the TF serving server will fail to start because it can not place those CPU only tensors correctly. The same case for when you want to customize GPU memory usage with Tensorflow Serving. If you don't want the server to allocate all memory on the default GPU device in the initial start, then you would need to use <span class=\"s1\"><code>tf.config.experimental.set_memory_growth</code></span> to ask Tensorflow to grow the memory usage when needed instead.</p>\n<p class=\"p1\">The solution? Use the <code>platform_config_file</code> argument. <span class=\"Apple-converted-space\">&nbsp;</span></p>\n<p class=\"p2\"></p>\n<h2 class=\"p1\">What is platform_config_file?</h2>\n<p class=\"p1\"><code>platform_config_file</code> is one of the arguments that Tensorflow Serving server provides. You can check out the argument in <a href=\"https://github.com/tensorflow/serving/blob/master/tensorflow_serving/model_servers/main.cc#L175\" rel=\"follow\">its source code</a>. As the source code explains, when provided with a <code>platform_config_file</code>:</p>\n<pre class=\"language-undefined\"><code>If non-empty, read an ascii PlatformConfigMap protobuf from the supplied file name, and use that platform config instead of the Tensorflow platform.</code></pre>\n<p class=\"p1\">Essentially, using <code>platform_config_file</code>, we are not using the arguments (most of them) that TF serving server provides for ease of use, but directly configuring the underlying Tensorflow configurations. This gives us the ability to directly configure Tensorflow session as well.</p>\n<p class=\"p1\">Let's take a look at an example of <code>platform_config_file</code>:</p>\n<pre class=\"language-undefined\"><code>platform_configs {\n  key: \"tensorflow\"\n  value {\n    source_adapter_config {\n      [type.googleapis.com/tensorflow.serving.SavedModelBundleSourceAdapterConfig] {\n        legacy_config {\n          session_config {\n            gpu_options {\n              per_process_gpu_memory_fraction: 0.4\n              allow_growth: true\n            }\n            allow_soft_placement: true\n          }\n          enable_model_warmup: true\n        }\n      }\n    }\n  }\n}</code></pre>\n<p class=\"p2\"></p>\n<p class=\"p2\">As you can see, in the <code>session_config</code> object here, we are able to provide all the typical Tensorflow session configurations just as we do when using standalone Tensorflow. In this case, using <code>platform_config_file</code>, those session configurations will be passed along to configure Tensorflow runtime session in Tensorflow Serving.&nbsp;</p>\n<p class=\"p2\">To use <code>platform_config_file</code> in Tensorflow Serving, what we need to do is to pass the argument and point it to your config file when starting Tensorflow Serving server:</p>\n<pre class=\"language-undefined\"><code>tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n  --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n  --platform_config_file=[PATH_TO_PLATFORM_CONFIG_FILE]</code></pre>\n<p></p>\n<h2 class=\"p1\">How to create a platform_config_file?</h2>\n<p>Creating a <code>platform_config_file</code> is very straightforward as well. There are 2 ways based on your preference.</p>\n<h3>The manual way</h3>\n<p>As shown in previous example of <code>platform_config_file</code>, the configurations are pretty straightforward, and you can easily add new fields/parameters. For all available parameters you can specify in the <code>platform_config_file</code>, checkout <a href=\"https://github.com/tensorflow/serving/blob/master/tensorflow_serving/servables/tensorflow/session_bundle_config.proto\" rel=\"follow\">the source code file</a>. A quick tip about the source code, its data format is called protocol buffer. It looks like JSON but it's not. Protocol Buffers (also called <strong>Protobuf</strong>) is a free and open source cross-platform library used to serialize structured data, developed by Google. It's not surprising that Tensorflow code base choose to use Protobuf, since it's also developed by Google.&nbsp;</p>\n<p>In short, to create your own <code>platform_config_file</code>, you can copy the example I have in the above section, and add/delete fields as you need. This is the manual way.</p>\n<p></p>\n<h3>The programmatic way</h3>\n<p>Now, if dealing with all the parameters in the source code seems overwhelming, there is a programmatic way to create your <code>platform_config_file</code>. Tensorflow provides a way to define all your session configurations using Tensorflow, and then convert them into a <code>platform_config_file</code>format. If that's something you want to explore, I recommend checking out <a href=\"https://gist.github.com/cutewalker/58e1c4f71b5af4822bc732fd619ebda3\" rel=\"follow\">this code snippet</a> for generating <code>platform_config_file</code>.</p>\n<p></p>\n<h2 class=\"p1\">Final Words</h2>\n<p>Not being able to configure Tensorflow runtime session when using Tensorflow Serving can be limiting, particularly when you are switching from standalone Tensorflow to Tensorflow Serving with various needs of configuring the runtime session or gpu options to fit your needs. In this article, we looked at how to use <code>platform_config_file</code> to address this limit and <span class=\"Apple-converted-space\">configure Tensorflow runtime session in Tensorflow Serving. Tensorflow Serving is still developing, hopefully in newer releases it will provide a way to more easily pass runtime Tensorflow session configurations.</span></p>\n<p><span class=\"Apple-converted-space\"></span></p>\n<p><span class=\"Apple-converted-space\"></span></p>","date":"6/29/2021","meta_description":"In this article, we looked at how to use platform_config_file to configure Tensorflow runtime session in Tensorflow Serving.","tags":[{"name":"Tensorflow Serving"},{"name":"Tensorflow"},{"name":"platform_config_file"},{"name":"platform config"},{"name":"session"}]}},"pageContext":{"slug":"use-platform-config-file-to-configure-tensorflow-session-in-tensorflow-serving"}},
    "staticQueryHashes": []}